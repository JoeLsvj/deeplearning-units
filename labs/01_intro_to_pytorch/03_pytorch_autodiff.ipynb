{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0e6e08a7368590a",
   "metadata": {},
   "source": [
    "# ðŸ”¥ Introduction to `PyTorch` -- Algorithmic Differentiation\n",
    "\n",
    "[Deep Learning](https://dsai.units.it/index.php/courses/deep-learning/) Course @ [UniTS](https://portale.units.it/en), Spring 2024\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/emaballarin/deeplearning-units/blob/main/labs/01_intro_to_pytorch/03_pytorch_autodiff.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>  <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/emaballarin/deeplearning-units/blob/main/labs/01_intro_to_pytorch/03_pytorch_autodiff.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecac3ac454a4e440",
   "metadata": {},
   "source": [
    "### Towards gradient-based model fitting\n",
    "\n",
    "In the previous notebook, we defined a simple linear model and to fitted it to data using its analytical OLS solution. In general, especially in complex cases, we cannot rely on analytical solutions and we need to resort to numerical optimization. Specifically, we would like to apply **gradient-based optimization** to fit our model to data.\n",
    "\n",
    "But first, we need to compute such gradients! ðŸ™ƒ\n",
    "\n",
    "PyTorch is built with differentiation in mind. Generally speaking, all of PyTorch built-in functions support algorithmic differentiability (unless the function is pathologically non-differentiable). This also includes functions that are not differentiable in the proper mathematical sense! For a more detailed discussion about differentiation in such cases, see [the official documentation](https://pytorch.org/docs/main/notes/autograd.html#gradients-for-non-differentiable-functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137b899a5ed39229",
   "metadata": {},
   "source": [
    "### Preliminary infrastucture setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "209cce0b2f377ddf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:24:49.865129Z",
     "start_time": "2024-03-05T01:24:49.859249Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:16.343012Z",
     "iopub.status.busy": "2024-03-05T01:29:16.342847Z",
     "iopub.status.idle": "2024-03-05T01:29:16.350157Z",
     "shell.execute_reply": "2024-03-05T01:29:16.349722Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "FOLDERNAME: str = \"deeplearning_units_2024\"\n",
    "try:\n",
    "    if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "        # noinspection PyUnresolvedReferences\n",
    "        from google.colab import drive\n",
    "\n",
    "        drive.mount(BASEPATH := \"/content/drive\")\n",
    "        os.makedirs(FULLPATH := BASEPATH + \"/MyDrive/\" + FOLDERNAME, exist_ok=True)\n",
    "    elif os.getenv(\"KAGGLE_CONTAINER_NAME\"):\n",
    "        os.makedirs(FULLPATH := \"/kaggle/working/\" + FOLDERNAME, exist_ok=True)\n",
    "    else:\n",
    "        os.makedirs(FULLPATH := \"./\" + FOLDERNAME, exist_ok=True)\n",
    "    os.chdir(FULLPATH)\n",
    "except (ModuleNotFoundError, FileExistsError, FileNotFoundError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5635bf19ace47f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:24:51.765126Z",
     "start_time": "2024-03-05T01:24:49.967665Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:16.352313Z",
     "iopub.status.busy": "2024-03-05T01:29:16.351916Z",
     "iopub.status.idle": "2024-03-05T01:29:19.128501Z",
     "shell.execute_reply": "2024-03-05T01:29:19.127559Z"
    }
   },
   "outputs": [],
   "source": [
    "!python -m pip install -q icecream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e4c99ffa3270ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:24:52.029505Z",
     "start_time": "2024-03-05T01:24:52.002467Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:19.131169Z",
     "iopub.status.busy": "2024-03-05T01:29:19.130978Z",
     "iopub.status.idle": "2024-03-05T01:29:19.155522Z",
     "shell.execute_reply": "2024-03-05T01:29:19.154916Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pretty printouts\n",
    "from icecream import ic\n",
    "\n",
    "ic.configureOutput(outputFunction=print)\n",
    "ic.configureOutput(prefix=\"    | \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27188a59391dcf0",
   "metadata": {},
   "source": [
    "### Some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e64f7d1efb5a589",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:24:54.495676Z",
     "start_time": "2024-03-05T01:24:52.031292Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:19.157700Z",
     "iopub.status.busy": "2024-03-05T01:29:19.157523Z",
     "iopub.status.idle": "2024-03-05T01:29:20.029784Z",
     "shell.execute_reply": "2024-03-05T01:29:20.029238Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch as th"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af70fc76132ebd9",
   "metadata": {},
   "source": [
    "### Let's differentiate!\n",
    "\n",
    "Practically speaking, each torch `Tensor` has a boolean attribute `requires_grad`, which tells the algorithmic differentiation engine (`autograd`) whether it should keep track of operations on the tensor or not -- for the purpose of computing gradients.\n",
    "\n",
    "When we perform operations on tensors with `requires_grad=True`, PyTorch builds a computational graph of operations on tensors. This graph is then used to compute the gradients of the final output with respect to the input tensors, using the chain rule of calculus (implemented efficiently, in *reverse mode*, by the ***backpropagation algorithm***)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bee50fea0e90ad51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:24:54.500871Z",
     "start_time": "2024-03-05T01:24:54.497205Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.032433Z",
     "iopub.status.busy": "2024-03-05T01:29:20.032212Z",
     "iopub.status.idle": "2024-03-05T01:29:20.034946Z",
     "shell.execute_reply": "2024-03-05T01:29:20.034439Z"
    }
   },
   "outputs": [],
   "source": [
    "x = th.rand(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a9326be5e047ee1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:24:54.512171Z",
     "start_time": "2024-03-05T01:24:54.502019Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.037182Z",
     "iopub.status.busy": "2024-03-05T01:29:20.036787Z",
     "iopub.status.idle": "2024-03-05T01:29:20.045995Z",
     "shell.execute_reply": "2024-03-05T01:29:20.045555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    | x.requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "_ = ic(x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1113650d00131d53",
   "metadata": {},
   "source": [
    "We can manually set the `requires_grad` attribute to `True`, or create directly a Tensor with it pre-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9750112c3aae0bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:24:54.517429Z",
     "start_time": "2024-03-05T01:24:54.513346Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.048087Z",
     "iopub.status.busy": "2024-03-05T01:29:20.047639Z",
     "iopub.status.idle": "2024-03-05T01:29:20.050274Z",
     "shell.execute_reply": "2024-03-05T01:29:20.049842Z"
    }
   },
   "outputs": [],
   "source": [
    "x.requires_grad_(\n",
    "    True\n",
    ")  # Note: usually, the underscore at the end of a method name in PyTorch means \"in-place\"\n",
    "\n",
    "# Or alternatively:\n",
    "y = th.rand(3, 3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "304412a51b4fec71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:24:54.526372Z",
     "start_time": "2024-03-05T01:24:54.518552Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.052162Z",
     "iopub.status.busy": "2024-03-05T01:29:20.051749Z",
     "iopub.status.idle": "2024-03-05T01:29:20.057525Z",
     "shell.execute_reply": "2024-03-05T01:29:20.057077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    | x.requires_grad: True, y.requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "_ = ic(x.requires_grad, y.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6f2a68e40384af8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:24:54.530685Z",
     "start_time": "2024-03-05T01:24:54.527501Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.059604Z",
     "iopub.status.busy": "2024-03-05T01:29:20.059226Z",
     "iopub.status.idle": "2024-03-05T01:29:20.061741Z",
     "shell.execute_reply": "2024-03-05T01:29:20.061306Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d36ff8945d0aed0",
   "metadata": {},
   "source": [
    "### Differentiation of functions\n",
    "\n",
    "Now, let's compute a derivative of a function!\n",
    "\n",
    "We consider first a function $f : \\mathbb{R} \\to \\mathbb{R}$, *e.g.* $f(x) = x^2$. We would like to compute its derivative at a given (randomly chosen) point $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bb63b8117933d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:24:54.537349Z",
     "start_time": "2024-03-05T01:24:54.533267Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.063521Z",
     "iopub.status.busy": "2024-03-05T01:29:20.063306Z",
     "iopub.status.idle": "2024-03-05T01:29:20.065831Z",
     "shell.execute_reply": "2024-03-05T01:29:20.065408Z"
    }
   },
   "outputs": [],
   "source": [
    "x = th.rand(1, requires_grad=True)\n",
    "y = x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20eccfdcf69cde87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:24:54.554068Z",
     "start_time": "2024-03-05T01:24:54.538541Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.067781Z",
     "iopub.status.busy": "2024-03-05T01:29:20.067318Z",
     "iopub.status.idle": "2024-03-05T01:29:20.075801Z",
     "shell.execute_reply": "2024-03-05T01:29:20.075345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    | x: tensor([0.3299], requires_grad=True)\n",
      "    | y: tensor([0.1088], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# In this specific case:\n",
    "_ = ic(x)\n",
    "_ = ic(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e691e85075a852f",
   "metadata": {},
   "source": [
    "To compute gradients, we call the `backward` method of the output tensor. This method computes the gradients of the output with respect to the input tensors. It is important to note that the `backward` method can only be called on a **scalar output tensor**. In this case, output tensor `y` is a scalar, so we can call `backward` on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fb786f38b31e790",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:24:56.033660Z",
     "start_time": "2024-03-05T01:24:54.555338Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.077646Z",
     "iopub.status.busy": "2024-03-05T01:29:20.077370Z",
     "iopub.status.idle": "2024-03-05T01:29:20.112696Z",
     "shell.execute_reply": "2024-03-05T01:29:20.112102Z"
    }
   },
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0a28e982adcbea",
   "metadata": {},
   "source": [
    "The gradient of $y$ with respect to $x$ is stored in the `grad` attribute of the input tensor `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b66dd4634f83d2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:24:56.044144Z",
     "start_time": "2024-03-05T01:24:56.035120Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.115115Z",
     "iopub.status.busy": "2024-03-05T01:29:20.114911Z",
     "iopub.status.idle": "2024-03-05T01:29:20.121825Z",
     "shell.execute_reply": "2024-03-05T01:29:20.121353Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    | x.grad: tensor([0.6597])\n"
     ]
    }
   ],
   "source": [
    "_ = ic(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e16ae2ab34446f",
   "metadata": {},
   "source": [
    "We can also check that the gradient is correct by computing the derivative of $f(x) = x^2$ analytically: $f'(x) = 2x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45c149d5433b1f78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:24:56.053808Z",
     "start_time": "2024-03-05T01:24:56.045427Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.123851Z",
     "iopub.status.busy": "2024-03-05T01:29:20.123561Z",
     "iopub.status.idle": "2024-03-05T01:29:20.128747Z",
     "shell.execute_reply": "2024-03-05T01:29:20.128317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    | 2 * x == x.grad: tensor([True])\n"
     ]
    }
   ],
   "source": [
    "_ = ic(2 * x == x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879ab886426a4458",
   "metadata": {},
   "source": [
    "The same can be applied to a function $f : \\mathbb{R}^d \\to \\mathbb{R}$, *e.g.* $f(\\mathbf{x}) = \\sum_{i=1}^d \\mathbf{x}_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de5920db339bceef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:24:56.060033Z",
     "start_time": "2024-03-05T01:24:56.054950Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.130592Z",
     "iopub.status.busy": "2024-03-05T01:29:20.130332Z",
     "iopub.status.idle": "2024-03-05T01:29:20.133158Z",
     "shell.execute_reply": "2024-03-05T01:29:20.132513Z"
    }
   },
   "outputs": [],
   "source": [
    "x = th.rand(3, requires_grad=True)\n",
    "y = x.sum()\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5f97a4fe4abb6e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:24:56.068880Z",
     "start_time": "2024-03-05T01:24:56.061400Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.134894Z",
     "iopub.status.busy": "2024-03-05T01:29:20.134697Z",
     "iopub.status.idle": "2024-03-05T01:29:20.143033Z",
     "shell.execute_reply": "2024-03-05T01:29:20.142544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    | x.grad: tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "_ = ic(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e562340b8f14d550",
   "metadata": {},
   "source": [
    "### Composed functions and the computational graph\n",
    "\n",
    "We can use also compute the gradient of a composition of functions. For our objective, it is useful to think of such composition in terms of a *computational graph*. That is, we can interpret the composed function $g \\circ f$ (such that $y = g(f(x))$) as a sequence of operations on tensors, which can be represented as a graph.\n",
    "\n",
    "Specifically:\n",
    "$z = f(x)$\n",
    "$y = g(z)$\n",
    "\n",
    "Which becomes, graphically:\n",
    "\n",
    "![](https://i.ibb.co/D74Gv43/compgra2.jpg)\n",
    "\n",
    "\\\n",
    "Now, let's suppose that $y = \\log^2(x)$, *i.e.*: $z=f(x)=\\log(x)$ and $y=g(z)=z^2$.\n",
    "\n",
    "We expect that $\\frac{dy}{dx}=2\\frac{\\log(x)}{|x|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b48cad72058fdffb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:24:56.079645Z",
     "start_time": "2024-03-05T01:24:56.069953Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.145236Z",
     "iopub.status.busy": "2024-03-05T01:29:20.144852Z",
     "iopub.status.idle": "2024-03-05T01:29:20.153831Z",
     "shell.execute_reply": "2024-03-05T01:29:20.153343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    | x: tensor([0.0533], requires_grad=True)\n",
      "      z: tensor([-2.9316], grad_fn=<LogBackward0>)\n",
      "      y: tensor([8.5942], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = th.rand(1, requires_grad=True)\n",
    "z = x.log()\n",
    "y = z**2\n",
    "\n",
    "_ = ic(x, z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e6aaf23971be3fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:24:56.087352Z",
     "start_time": "2024-03-05T01:24:56.080819Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.155867Z",
     "iopub.status.busy": "2024-03-05T01:29:20.155598Z",
     "iopub.status.idle": "2024-03-05T01:29:20.161230Z",
     "shell.execute_reply": "2024-03-05T01:29:20.160679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    | x.grad == 2 * x.log() / x: tensor([True])\n"
     ]
    }
   ],
   "source": [
    "# z.retain_grad()    # Ignore this for now, we will discuss it later...\n",
    "y.backward()\n",
    "_ = ic(x.grad == 2 * x.log() / x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780503b46060f216",
   "metadata": {},
   "source": [
    "Now, suppose we want to access $\\frac{dy}{dz}=2z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a49af400fc28492",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:24:56.097246Z",
     "start_time": "2024-03-05T01:24:56.088306Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.163284Z",
     "iopub.status.busy": "2024-03-05T01:29:20.162867Z",
     "iopub.status.idle": "2024-03-05T01:29:20.169771Z",
     "shell.execute_reply": "2024-03-05T01:29:20.169357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    | z.grad: None\n",
      "    | z.grad == 2 * z: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26766/1087612663.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1709539277183/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  _ = ic(z.grad)\n",
      "/tmp/ipykernel_26766/1087612663.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1709539277183/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  _ = ic(z.grad == 2 * z)\n"
     ]
    }
   ],
   "source": [
    "_ = ic(z.grad)\n",
    "_ = ic(z.grad == 2 * z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256e53122b426a98",
   "metadata": {},
   "source": [
    "As the error message suggests, we need to explicitly retain the gradient on non-leaf (roughly: neither input nor output) tensors. This is done for memory-efficiency reasons, as storing gradients for all tensors in the computational graph would be wasteful in typical scenarios. We can do this by calling the `retain_grad` method on the tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a26a4b055e7196",
   "metadata": {},
   "source": [
    "### `None` gradients, gradient accumulation, and graph retention\n",
    "\n",
    "Before the `backward` method is called, the gradient of the output tensor with respect to the input tensors is `None`. This is also the case for non-retained intermediate tensors in the computational graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2cd3d07bb654a40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:24:56.101432Z",
     "start_time": "2024-03-05T01:24:56.098457Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.171612Z",
     "iopub.status.busy": "2024-03-05T01:29:20.171290Z",
     "iopub.status.idle": "2024-03-05T01:29:20.174113Z",
     "shell.execute_reply": "2024-03-05T01:29:20.173672Z"
    }
   },
   "outputs": [],
   "source": [
    "x = th.rand(1, requires_grad=True)\n",
    "z = x.log()\n",
    "y = z**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d7fc66eba059bac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:24:56.109600Z",
     "start_time": "2024-03-05T01:24:56.102532Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.175927Z",
     "iopub.status.busy": "2024-03-05T01:29:20.175799Z",
     "iopub.status.idle": "2024-03-05T01:29:20.180494Z",
     "shell.execute_reply": "2024-03-05T01:29:20.179886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    | x.grad: None, z.grad: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26766/1827219229.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1709539277183/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  _ = ic(x.grad, z.grad)\n"
     ]
    }
   ],
   "source": [
    "_ = ic(x.grad, z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97119eab367268f1",
   "metadata": {},
   "source": [
    "If we call `backward` on `y`, the gradient of `y` with respect to `x` is computed and stored in `x.grad`. However, if we call `backward` again, the gradients accumulate (*i.e.*, sum). This may become useful in some scenarios. If we want to reset the gradient, we must zero it or set it to None manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ebc5dbd4bb1c25d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:24:56.118084Z",
     "start_time": "2024-03-05T01:24:56.110627Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.182029Z",
     "iopub.status.busy": "2024-03-05T01:29:20.181906Z",
     "iopub.status.idle": "2024-03-05T01:29:20.187499Z",
     "shell.execute_reply": "2024-03-05T01:29:20.186996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    | x.grad: tensor([-1.6456])\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "_ = ic(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b634463dfde5f3e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:24:56.121551Z",
     "start_time": "2024-03-05T01:24:56.119046Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.189204Z",
     "iopub.status.busy": "2024-03-05T01:29:20.189013Z",
     "iopub.status.idle": "2024-03-05T01:29:20.191453Z",
     "shell.execute_reply": "2024-03-05T01:29:20.191039Z"
    }
   },
   "outputs": [],
   "source": [
    "# We must re-define our graph, otherwise...\n",
    "z = x.log()\n",
    "y = z**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17266bc4afcd1e21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:24:56.129572Z",
     "start_time": "2024-03-05T01:24:56.122565Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.193294Z",
     "iopub.status.busy": "2024-03-05T01:29:20.192780Z",
     "iopub.status.idle": "2024-03-05T01:29:20.197686Z",
     "shell.execute_reply": "2024-03-05T01:29:20.197265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    | x.grad: tensor([-3.2911])\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "_ = ic(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25700b56aa68e1a",
   "metadata": {},
   "source": [
    "What would happen if we did not re-define the graph, but we called `backward` on `y` again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fab6938749f6ba2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:25:31.246060Z",
     "start_time": "2024-03-05T01:25:31.237763Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.199504Z",
     "iopub.status.busy": "2024-03-05T01:29:20.199127Z",
     "iopub.status.idle": "2024-03-05T01:29:20.206205Z",
     "shell.execute_reply": "2024-03-05T01:29:20.205788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUPPRESSED RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    y.backward()\n",
    "except RuntimeError as e:\n",
    "    print(f\"SUPPRESSED RuntimeError: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5efc38f8dc462f",
   "metadata": {},
   "source": [
    "To avoid this, we can set the `retain_graph` argument of `backward` to `True`. This will retain the computational graph, so that we can call `backward` multiple times without re-defining the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21e9180ef9dbbd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:25:34.226666Z",
     "start_time": "2024-03-05T01:25:34.216558Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.207837Z",
     "iopub.status.busy": "2024-03-05T01:29:20.207628Z",
     "iopub.status.idle": "2024-03-05T01:29:20.214226Z",
     "shell.execute_reply": "2024-03-05T01:29:20.213697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    | x.grad: tensor([-4.9367])\n"
     ]
    }
   ],
   "source": [
    "z = x.log()\n",
    "y = z**2\n",
    "y.backward(retain_graph=True)\n",
    "_ = ic(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7422a2a8002ae32e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:25:34.711049Z",
     "start_time": "2024-03-05T01:25:34.706797Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.216006Z",
     "iopub.status.busy": "2024-03-05T01:29:20.215774Z",
     "iopub.status.idle": "2024-03-05T01:29:20.218209Z",
     "shell.execute_reply": "2024-03-05T01:29:20.217759Z"
    }
   },
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec7f26074952e0f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:25:35.334688Z",
     "start_time": "2024-03-05T01:25:35.325552Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.219972Z",
     "iopub.status.busy": "2024-03-05T01:29:20.219570Z",
     "iopub.status.idle": "2024-03-05T01:29:20.224980Z",
     "shell.execute_reply": "2024-03-05T01:29:20.224565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    | x.grad: tensor([-6.5823])\n"
     ]
    }
   ],
   "source": [
    "_ = ic(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9688038c471791e",
   "metadata": {},
   "source": [
    "### Disabling gradient tracking\n",
    "\n",
    "We can also disable gradient tracking by wrapping the computation in a `with th.no_grad():` block. This is useful when we are only interested in *forward* computation, and we want to save memory and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d2db2edf10184b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:25:36.831323Z",
     "start_time": "2024-03-05T01:25:36.826316Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.226869Z",
     "iopub.status.busy": "2024-03-05T01:29:20.226335Z",
     "iopub.status.idle": "2024-03-05T01:29:20.229382Z",
     "shell.execute_reply": "2024-03-05T01:29:20.228974Z"
    }
   },
   "outputs": [],
   "source": [
    "x = th.rand(1, requires_grad=True)\n",
    "with th.no_grad():\n",
    "    y = x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c768d669c392628",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T01:25:37.246771Z",
     "start_time": "2024-03-05T01:25:37.237929Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-05T01:29:20.231151Z",
     "iopub.status.busy": "2024-03-05T01:29:20.230796Z",
     "iopub.status.idle": "2024-03-05T01:29:20.237397Z",
     "shell.execute_reply": "2024-03-05T01:29:20.236953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUPPRESSED RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
      "\n",
      "    | x.grad: None\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    y.backward()\n",
    "except RuntimeError as e:\n",
    "    print(f\"SUPPRESSED RuntimeError: {e}\\n\")\n",
    "_ = ic(x.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
